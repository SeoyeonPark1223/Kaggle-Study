{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Aerial Cactus Identification - Modeling ","metadata":{}},{"cell_type":"markdown","source":"Tutorial Link -> https://www.kaggle.com/code/werooring/ch11-modeling","metadata":{}},{"cell_type":"markdown","source":"- Performance Improvement\n    - Perform various image transformation -> define image transformer\n    - Create deeper CNN\n    - Use better optimizer -> instead of standard one\n    - Increase epoch number ","metadata":{}},{"cell_type":"markdown","source":"**Fix Seed Value**","metadata":{}},{"cell_type":"code","source":"import torch # pytorch\nimport random\nimport numpy as np\nimport os\n\n# fix seed value\nseed = 50\nos.environ['PYTHONHASHSEED'] = str(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed) \ntorch.backends.cudnn.deterministic = True \ntorch.backends.cudnn.benchmark = False    \ntorch.backends.cudnn.enabled = False      ","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:14:31.374690Z","iopub.execute_input":"2024-09-10T02:14:31.375017Z","iopub.status.idle":"2024-09-10T02:14:34.672693Z","shell.execute_reply.started":"2024-09-10T02:14:31.374983Z","shell.execute_reply":"2024-09-10T02:14:34.671728Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:14:34.674788Z","iopub.execute_input":"2024-09-10T02:14:34.675623Z","iopub.status.idle":"2024-09-10T02:14:34.759413Z","shell.execute_reply.started":"2024-09-10T02:14:34.675577Z","shell.execute_reply":"2024-09-10T02:14:34.757478Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:14:46.028466Z","iopub.execute_input":"2024-09-10T02:14:46.029298Z","iopub.status.idle":"2024-09-10T02:14:46.036277Z","shell.execute_reply.started":"2024-09-10T02:14:46.029252Z","shell.execute_reply":"2024-09-10T02:14:46.035280Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-10T02:14:34.765509Z","iopub.execute_input":"2024-09-10T02:14:34.768152Z","iopub.status.idle":"2024-09-10T02:14:35.172347Z","shell.execute_reply.started":"2024-09-10T02:14:34.768109Z","shell.execute_reply":"2024-09-10T02:14:35.171294Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/aerial-cactus-identification/sample_submission.csv\n/kaggle/input/aerial-cactus-identification/train.zip\n/kaggle/input/aerial-cactus-identification/test.zip\n/kaggle/input/aerial-cactus-identification/train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"data_path = '/kaggle/input/aerial-cactus-identification/'\n\nlabels = pd.read_csv(data_path + 'train.csv') # train data \nsubmission = pd.read_csv(data_path + 'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:15:41.652200Z","iopub.execute_input":"2024-09-10T02:15:41.652595Z","iopub.status.idle":"2024-09-10T02:15:41.797924Z","shell.execute_reply.started":"2024-09-10T02:15:41.652556Z","shell.execute_reply":"2024-09-10T02:15:41.797040Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# unzip the zip file \n\nfrom zipfile import ZipFile\n\n# unzip training img data\nwith ZipFile(data_path + 'train.zip') as zipper:\n    zipper.extractall()\n\n# unzip test img data \nwith ZipFile(data_path + 'test.zip') as zipper:\n    zipper.extractall()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:16:01.154900Z","iopub.execute_input":"2024-09-10T02:16:01.155293Z","iopub.status.idle":"2024-09-10T02:16:04.100462Z","shell.execute_reply.started":"2024-09-10T02:16:01.155254Z","shell.execute_reply":"2024-09-10T02:16:04.099462Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Divide Train / Validation Data**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, valid = train_test_split(labels, \n                               test_size = 0.1, # ratio; train : valid = 9 : 1\n                               stratify = labels['has_cactus'],\n                               random_state = 50)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:16:22.172516Z","iopub.execute_input":"2024-09-10T02:16:22.172918Z","iopub.status.idle":"2024-09-10T02:16:22.762727Z","shell.execute_reply.started":"2024-09-10T02:16:22.172880Z","shell.execute_reply":"2024-09-10T02:16:22.761929Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Define DataSet Class**","metadata":{}},{"cell_type":"code","source":"import cv2\nfrom torch.utils.data import Dataset \n\nclass ImageDataset(Dataset):\n    # constructor\n    def __init__(self, df, img_dir = './', transform = None):\n        super().__init__()\n        self.df = df # train or validation dataset \n        self.img_dir = img_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_id = self.df.iloc[idx, 0]\n        img_path = self.img_dir + img_id\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = self.df.iloc[idx, 1] # target value\n        \n        if self.transform is not None:\n            # if there's a transformer(변환기)\n            image = self.transform(image)\n            \n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:17:35.500711Z","iopub.execute_input":"2024-09-10T02:17:35.501287Z","iopub.status.idle":"2024-09-10T02:17:35.680785Z","shell.execute_reply.started":"2024-09-10T02:17:35.501247Z","shell.execute_reply":"2024-09-10T02:17:35.679800Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**Define Image Transformer**","metadata":{}},{"cell_type":"markdown","source":"- Why do we need to transform image? \n    - To create more image data -> Data augmentation (데이터 증강)","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms \n\ntransform_train = transforms.Compose([transforms.ToTensor(), # make image to Tensor object\n                                     transforms.Pad(32, padding_mode = 'symmetric'),\n                                     transforms.RandomHorizontalFlip(), # default: randomly choose 50% of image and transform symmetrically\n                                     transforms.RandomVerticalFlip(),\n                                     transforms.RandomRotation(10), # rotate randomly between -10~10 degrees\n                                     transforms.Normalize((0.485, 0.456, 0.406), # mean(R,G,B)\n                                                         (0.229, 0.224, 0.225))]) # distribution(R,G,B)\n\ntransform_test = transforms.Compose([transforms.ToTensor(),\n                                    transforms.Pad(32, padding_mode = 'symmetric'),\n                                    transforms.Normalize((0.485, 0.456, 0.406),\n                                                        (0.229, 0.224, 0.225))])","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:33:14.427183Z","iopub.execute_input":"2024-09-10T02:33:14.427771Z","iopub.status.idle":"2024-09-10T02:33:14.435207Z","shell.execute_reply.started":"2024-09-10T02:33:14.427719Z","shell.execute_reply":"2024-09-10T02:33:14.434270Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Create Dataset and Data Loader**","metadata":{}},{"cell_type":"code","source":"# create train / validation dataset\ndataset_train = ImageDataset(df = train, img_dir = 'train/', transform = transform_train)\ndataset_valid = ImageDataset(df = valid, img_dir = 'train/', transform = transform_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:34:14.899141Z","iopub.execute_input":"2024-09-10T02:34:14.900173Z","iopub.status.idle":"2024-09-10T02:34:14.905526Z","shell.execute_reply.started":"2024-09-10T02:34:14.900110Z","shell.execute_reply":"2024-09-10T02:34:14.904294Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nloader_train = DataLoader(dataset = dataset_train, batch_size = 32, shuffle = True)\nloader_valid = DataLoader(dataset = dataset_valid, batch_size = 32, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:34:50.309381Z","iopub.execute_input":"2024-09-10T02:34:50.309764Z","iopub.status.idle":"2024-09-10T02:34:50.314976Z","shell.execute_reply.started":"2024-09-10T02:34:50.309710Z","shell.execute_reply":"2024-09-10T02:34:50.314032Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Create Model","metadata":{}},{"cell_type":"markdown","source":"- Improved CNN model\n    - Deeper CNN\n    - Apply batch normalization \n        - Goal: Stabilize training, Faster convergence\n        - For each mini-batch, batch normalization computes the mean and variance of the inputs and normalizes them\n        - After normalization, batch normalization introduces two trainable parameters,  \\gamma  (scale) and  \\beta  (shift), to allow the network to restore the representational capacity if needed\n    - Use Leaky ReLU for activation function ","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__() # successed nn.Module's __init__() method call \n        \n        self.layer1 = nn.Sequential(nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, padding = 2),\n                                   nn.BatchNorm2d(32), # batch normalization \n                                   nn.LeakyReLU(), # Leaky ReLU for activation function\n                                   nn.MaxPool2d(kernel_size = 2))\n        \n        self.layer2 = nn.Sequential(nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 2),\n                                   nn.BatchNorm2d(64),\n                                   nn.LeakyReLU(),\n                                   nn.MaxPool2d(kernel_size = 2))\n        \n        self.layer3 = nn.Sequential(nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 2),\n                                   nn.BatchNorm2d(128),\n                                   nn.LeakyReLU(),\n                                   nn.MaxPool2d(kernel_size = 2))\n        \n        self.layer4 = nn.Sequential(nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 2),\n                                   nn.BatchNorm2d(256),\n                                   nn.LeakyReLU(),\n                                   nn.MaxPool2d(kernel_size = 2))\n        \n        self.layer5 = nn.Sequential(nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 2),\n                                   nn.BatchNorm2d(512),\n                                   nn.LeakyReLU(),\n                                   nn.MaxPool2d(kernel_size = 2))\n        \n        self.avg_pool = nn.AvgPool2d(kernel_size = 4)\n        \n        self.fc1 = nn.Linear(in_features = 512 * 1 * 1, out_features = 64)\n        self.fc2 = nn.Linear(in_features = 64, out_features = 2)\n        \n    # forward propagation\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.avg_pool(x)\n        x = x.view(-1, 512 * 1 *1) # flattening\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:50:16.234825Z","iopub.execute_input":"2024-09-10T02:50:16.235482Z","iopub.status.idle":"2024-09-10T02:50:16.248761Z","shell.execute_reply.started":"2024-09-10T02:50:16.235434Z","shell.execute_reply":"2024-09-10T02:50:16.247822Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = Model().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:50:37.632902Z","iopub.execute_input":"2024-09-10T02:50:37.633311Z","iopub.status.idle":"2024-09-10T02:50:37.844200Z","shell.execute_reply.started":"2024-09-10T02:50:37.633272Z","shell.execute_reply":"2024-09-10T02:50:37.843319Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Train Model","metadata":{}},{"cell_type":"markdown","source":"**Set Loss Function & Optimizer**","metadata":{}},{"cell_type":"code","source":"# loss function -> use cross entropy (because it's classification problem)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:51:23.775672Z","iopub.execute_input":"2024-09-10T02:51:23.776122Z","iopub.status.idle":"2024-09-10T02:51:23.782073Z","shell.execute_reply.started":"2024-09-10T02:51:23.776066Z","shell.execute_reply":"2024-09-10T02:51:23.780959Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# optimizer -> finding optimized weight algorithm\n# Adamax (better ver of Adam)\n\noptimizer = torch.optim.Adamax(model.parameters(), lr = 0.00006)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:52:40.487604Z","iopub.execute_input":"2024-09-10T02:52:40.488307Z","iopub.status.idle":"2024-09-10T02:52:40.493201Z","shell.execute_reply.started":"2024-09-10T02:52:40.488265Z","shell.execute_reply":"2024-09-10T02:52:40.492199Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**Train Model**","metadata":{}},{"cell_type":"code","source":"epochs = 70 # increase epoch to 70\n\nfor epoch in range(epochs):\n    epoch_loss = 0\n    \n    for images, labels in loader_train: # repeat count = len(loader_train)\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(images)\n        \n        loss = criterion(outputs, labels)\n        \n        epoch_loss += loss.item()\n        loss.backward()\n        \n        optimizer.step() # new weight = original weight - (learning rate * gradient)\n        \n    print(f'epoch [{epoch+1}/{epochs}] - loss: {epoch_loss/len(loader_train):.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:53:42.845203Z","iopub.execute_input":"2024-09-10T02:53:42.846244Z","iopub.status.idle":"2024-09-10T03:38:06.407917Z","shell.execute_reply.started":"2024-09-10T02:53:42.846191Z","shell.execute_reply":"2024-09-10T03:38:06.406887Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"epoch [1/70] - loss: 0.1302\nepoch [2/70] - loss: 0.0731\nepoch [3/70] - loss: 0.0572\nepoch [4/70] - loss: 0.0502\nepoch [5/70] - loss: 0.0424\nepoch [6/70] - loss: 0.0348\nepoch [7/70] - loss: 0.0308\nepoch [8/70] - loss: 0.0305\nepoch [9/70] - loss: 0.0286\nepoch [10/70] - loss: 0.0234\nepoch [11/70] - loss: 0.0211\nepoch [12/70] - loss: 0.0243\nepoch [13/70] - loss: 0.0199\nepoch [14/70] - loss: 0.0186\nepoch [15/70] - loss: 0.0176\nepoch [16/70] - loss: 0.0183\nepoch [17/70] - loss: 0.0176\nepoch [18/70] - loss: 0.0156\nepoch [19/70] - loss: 0.0170\nepoch [20/70] - loss: 0.0165\nepoch [21/70] - loss: 0.0160\nepoch [22/70] - loss: 0.0140\nepoch [23/70] - loss: 0.0142\nepoch [24/70] - loss: 0.0124\nepoch [25/70] - loss: 0.0140\nepoch [26/70] - loss: 0.0126\nepoch [27/70] - loss: 0.0128\nepoch [28/70] - loss: 0.0130\nepoch [29/70] - loss: 0.0106\nepoch [30/70] - loss: 0.0111\nepoch [31/70] - loss: 0.0132\nepoch [32/70] - loss: 0.0096\nepoch [33/70] - loss: 0.0110\nepoch [34/70] - loss: 0.0107\nepoch [35/70] - loss: 0.0097\nepoch [36/70] - loss: 0.0098\nepoch [37/70] - loss: 0.0095\nepoch [38/70] - loss: 0.0091\nepoch [39/70] - loss: 0.0087\nepoch [40/70] - loss: 0.0073\nepoch [41/70] - loss: 0.0080\nepoch [42/70] - loss: 0.0071\nepoch [43/70] - loss: 0.0077\nepoch [44/70] - loss: 0.0086\nepoch [45/70] - loss: 0.0087\nepoch [46/70] - loss: 0.0085\nepoch [47/70] - loss: 0.0075\nepoch [48/70] - loss: 0.0067\nepoch [49/70] - loss: 0.0067\nepoch [50/70] - loss: 0.0063\nepoch [51/70] - loss: 0.0067\nepoch [52/70] - loss: 0.0067\nepoch [53/70] - loss: 0.0064\nepoch [54/70] - loss: 0.0064\nepoch [55/70] - loss: 0.0048\nepoch [56/70] - loss: 0.0081\nepoch [57/70] - loss: 0.0050\nepoch [58/70] - loss: 0.0054\nepoch [59/70] - loss: 0.0053\nepoch [60/70] - loss: 0.0058\nepoch [61/70] - loss: 0.0053\nepoch [62/70] - loss: 0.0052\nepoch [63/70] - loss: 0.0061\nepoch [64/70] - loss: 0.0044\nepoch [65/70] - loss: 0.0045\nepoch [66/70] - loss: 0.0057\nepoch [67/70] - loss: 0.0039\nepoch [68/70] - loss: 0.0062\nepoch [69/70] - loss: 0.0036\nepoch [70/70] - loss: 0.0038\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Performance Validation","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nimport numpy as np\n\ntrue_list = []\npreds_list = []","metadata":{"execution":{"iopub.status.busy":"2024-09-10T03:40:52.987995Z","iopub.execute_input":"2024-09-10T03:40:52.989039Z","iopub.status.idle":"2024-09-10T03:40:52.993831Z","shell.execute_reply.started":"2024-09-10T03:40:52.988991Z","shell.execute_reply":"2024-09-10T03:40:52.992750Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model.eval() # evaluation stage -> won't apply dropout \n\nwith torch.no_grad(): # inactivate calculating gradient (no need to calculate gradient in evaluation step)\n    for images, labels in loader_valid:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        # why back to cpu? -> roc_auc is sklearn -> it can't perform on GPU\n        preds = torch.softmax(outputs.cpu(), dim = 1)[:, 1] # preds probability\n        true = labels.cpu() # true val\n        \n        # have to convert preds and true tensors to original python array or numpy array\n        preds_list.extend(preds.tolist())\n        true_list.extend(true.tolist())\n        \nprint(f'validation data ROC AUC: {roc_auc_score(true_list, preds_list):.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T03:41:27.393894Z","iopub.execute_input":"2024-09-10T03:41:27.394268Z","iopub.status.idle":"2024-09-10T03:41:29.886124Z","shell.execute_reply.started":"2024-09-10T03:41:27.394233Z","shell.execute_reply":"2024-09-10T03:41:29.885184Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"validation data ROC AUC: 0.9997\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prediction and Submit Result","metadata":{}},{"cell_type":"code","source":"# create test dataset and data loader\ndataset_test = ImageDataset(df = submission, img_dir = 'test/', transform = transform_test)\nloader_test = DataLoader(dataset = dataset_test, batch_size = 32, shuffle = False)\n\nmodel.eval()\n\npreds = []\n\nwith torch.no_grad():\n    for images, _ in loader_test:\n        images = images.to(device)\n        \n        outputs = model(images)\n        \n        preds_part = torch.softmax(outputs.cpu(), dim = 1)[:, 1].tolist()\n        \n        preds.extend(preds_part)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T03:42:34.640804Z","iopub.execute_input":"2024-09-10T03:42:34.641584Z","iopub.status.idle":"2024-09-10T03:42:40.027690Z","shell.execute_reply.started":"2024-09-10T03:42:34.641542Z","shell.execute_reply":"2024-09-10T03:42:40.026556Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"submission['has_cactus'] = preds\nsubmission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T03:42:46.929285Z","iopub.execute_input":"2024-09-10T03:42:46.930240Z","iopub.status.idle":"2024-09-10T03:42:46.954212Z","shell.execute_reply.started":"2024-09-10T03:42:46.930197Z","shell.execute_reply":"2024-09-10T03:42:46.953315Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import shutil\n\n# delete entire directory \nshutil.rmtree('./train')\nshutil.rmtree('./test')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T03:42:53.594173Z","iopub.execute_input":"2024-09-10T03:42:53.595045Z","iopub.status.idle":"2024-09-10T03:42:54.232108Z","shell.execute_reply.started":"2024-09-10T03:42:53.595002Z","shell.execute_reply":"2024-09-10T03:42:54.231273Z"},"trusted":true},"execution_count":26,"outputs":[]}]}